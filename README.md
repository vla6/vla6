## Valerie Carey's Personal Repository 

This is my personal code stash, where I explore some of my favorite topics, including categorical feature embeddings and encodings, model explainaibiltu methods and their limitations, and testing processes to detect and mitigate bias in machine learning models.

*See also [vcarey-circlestar](https://github.com/vcarey-circlestar)*

## Towards Data Science 

You're a  Towards Data Science Reader?  Get the code used for my articles here:

#### [Visualizing Stochastic Regularization for Entity Embeddings](https://github.com/vla6/Blog_naics_nn/tree/main/_A_embeddings)

Pretty pictures illustrate how stochastic regularization impacts entity embeddings in neural network models.

Read the article at [TDS](https://towardsdatascience.com/visualizing-stochastic-regularization-for-entity-embeddings-c0109ced4a3a/) or [Medium](https://medium.com/data-science/visualizing-stochastic-regularization-for-entity-embeddings-c0109ced4a3a).

#### [Data Disruptions to Elevate Entity Embeddings](https://github.com/vla6/Blog_naics_nn)

Exploring how messing with data can increase model accuracy and robustness, when using entity embeddings in neural network models.

Read the article at [TDS](https://towardsdatascience.com/data-disruptions-to-elevate-entity-embeddings-b1ddf86a3c95/) or [Medium](https://towardsdatascience.com/data-disruptions-to-elevate-entity-embeddings-b1ddf86a3c95).


#### [No Label Left Behind: Alternative Encodings for Hierarchical Categoricals](https://github.com/vla6/Blog_gnn_naics/tree/main/A_target_count_encoding)

A comparison of several methods of encoding categorical features for XGBoost machine learning models.  Over-engineering of features can be problematic for missing or unseen codes.  Results suggest that simpler methods, in conjunction with data manipulation, may work better for changing business environments or code sets that update frequently. 

Read the article at [TDS](https://towardsdatascience.com/no-label-left-behind-alternative-encodings-for-hierarchical-categoricals-d1bcf00afc37/) or [Medium](https://medium.com/data-science/no-label-left-behind-alternative-encodings-for-hierarchical-categoricals-d1bcf00afc37).

#### [Exploring Hierarchical Blending in Target Encoding](https://github.com/vla6/Blog_gnn_naics)

In which I test a proposed method for encoding categorical features for machine learning models, e.g. XGBoost and neural networks, and discover tradeoffs between performance and robustness, which suggest alternative ways to approach these features.

Read the article at [TDS](https://medium.com/data-science/exploring-hierarchical-blending-in-target-encoding-fea4c59b305b) or [Medium](https://medium.com/data-science/no-label-left-behind-alternative-encodings-for-hierarchical-categoricals-d1bcf00afc37).

#### [SHAP vs. ALE for Feature Interactions: Understanding Conflicting Results](https://github.com/vla6/Blog_gnn_naics)

A deep dive into why two popular model explainability methods, SHAP and ALE, show opposite results in a public dataset.  This leads me to go on about how model "explainers" don't produce "explanations".  Instead, these methods should be viewed as diagnostic tests which must be interpreted thoughtfully.

Read the article at [TDS](https://towardsdatascience.com/shap-vs-ale-for-feature-interactions-understanding-conflicting-results-ac506149f678/) or [Medium](hhttps://medium.com/data-science/shap-vs-ale-for-feature-interactions-understanding-conflicting-results-ac506149f678).


<!--
**vla6/vla6** is a âœ¨ _special_ âœ¨ repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- ðŸ”­ Iâ€™m currently working on ...
- ðŸŒ± Iâ€™m currently learning ...
- ðŸ‘¯ Iâ€™m looking to collaborate on ...
- ðŸ¤” Iâ€™m looking for help with ...
- ðŸ’¬ Ask me about ...
- ðŸ“« How to reach me: ...
- ðŸ˜„ Pronouns: ...
- âš¡ Fun fact: ...
-->
